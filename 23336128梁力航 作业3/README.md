# 机器学习作业：线性回归和逻辑回归

本项目实现了线性回归和逻辑回归算法，用于完成机器学习课程的作业3。

## 项目结构

```
.
├── utils.py                          # 工具函数（数据加载、预处理）
├── linear_regression.py              # 线性回归模型实现
├── logistic_regression.py            # 逻辑回归模型实现
├── visualization.py                  # 可视化函数
├── exercise_one.py                   # 练习一主程序（线性回归）
├── exercise_two.py                   # 练习二主程序（逻辑回归）
├── analysis/                         # 生成的图表存储目录
├── dataForTrainingLinear.txt         # 线性回归训练数据
├── dataForTestingLinear.txt          # 线性回归测试数据
├── dataForTrainingLogistic.txt       # 逻辑回归训练数据
├── dataForTestingLogistic.txt        # 逻辑回归测试数据
└── README.md                         # 本文件
```

## 依赖环境

本项目需要以下Python库：

- Python 3.x
- NumPy (数值计算)
- Matplotlib (数据可视化)

### 安装依赖

```bash
pip install numpy matplotlib
```

或使用conda：

```bash
conda install numpy matplotlib
```

## 使用方法

### 练习一：线性回归

运行线性回归实验：

```bash
python exercise_one.py
```

该程序将执行以下任务：

1. **(a) 批量梯度下降训练**
   - 学习率: 0.00015
   - 迭代次数: 1,500,000
   - 每100,000次迭代记录训练和测试误差
   - 生成误差曲线图

2. **(b) 不同学习率分析**
   - 学习率: 0.0002
   - 比较不同学习率对收敛的影响

3. **(c) 随机梯度下降训练**
   - 实现SGD算法
   - 比较SGD与批量梯度下降的收敛行为

**输出：**
- 最优参数值
- 训练和测试误差
- 误差曲线图（保存在 `analysis/` 目录）
- 分析结果和观察

### 练习二：逻辑回归

运行逻辑回归实验：

```bash
python exercise_two.py
```

该程序将执行以下任务：

1. **(a) 数学推导**
   - 输出条件对数似然公式

2. **(b) 梯度推导**
   - 输出偏导数公式

3. **(c) 训练逻辑回归分类器**
   - 使用随机梯度上升训练
   - 输出最优参数

4. **(d) 测试集评估**
   - 报告误分类样本数
   - 计算准确率

5. **(e) 收敛分析**
   - 绘制对数似然收敛曲线
   - 报告收敛所需迭代次数

6. **(f) 训练集大小分析**
   - 分析训练集大小对性能的影响
   - 绘制训练误差和测试误差随训练集大小的变化

**输出：**
- 数学公式推导
- 最优参数值
- 测试集误分类数和准确率
- 收敛曲线图
- 训练集大小分析图（保存在 `analysis/` 目录）

## 算法说明

### 线性回归

**模型：** y = w₀ + w₁x₁ + w₂x₂

**损失函数（MSE）：**
```
J(w) = (1/2n) Σ(h_w(x^(i)) - y^(i))²
```

**梯度：**
```
∂J/∂w_j = (1/n) Σ(h_w(x^(i)) - y^(i)) * x_j^(i)
```

**参数更新（梯度下降）：**
```
w_j := w_j - α * ∂J/∂w_j
```

### 逻辑回归

**模型：** h_w(x) = σ(w^T x) = 1 / (1 + e^(-w^T x))

**目标函数（对数似然）：**
```
ℓ(w) = Σ[y^(i) * log(h_w(x^(i))) + (1-y^(i)) * log(1-h_w(x^(i)))]
```

**梯度：**
```
∂ℓ/∂w_j = Σ(y^(i) - h_w(x^(i))) * x_j^(i)
```

**参数更新（梯度上升）：**
```
w_j := w_j + α * ∂ℓ/∂w_j
```

## 数据说明

### 线性回归数据

**dataForTrainingLinear.txt / dataForTestingLinear.txt**
- 列1: 房屋面积（平方米）
- 列2: 距离双鸭山职业技术学院的距离（千米）
- 列3: 房价（亿元）

### 逻辑回归数据

**dataForTrainingLogistic.txt / dataForTestingLogistic.txt**
- 列1-6: 特征
- 列7: 标签（0或1）

## 结果分析

### 线性回归关键发现

1. **梯度下降收敛**：训练误差和测试误差随迭代次数增加而下降，最终趋于稳定
2. **学习率影响**：较大的学习率收敛更快，但过大可能导致震荡或发散
3. **SGD vs 批量GD**：SGD误差曲线更波动，但最终收敛结果相近

#### 关于测试误差的重要说明

**问题**: 为什么测试误差（133.75）比训练误差（3.82）大这么多？

**答案**: 这是正常现象！

- **训练误差 MSE = 3.82** → 平均预测误差 ≈ 1.95亿元（相对误差0.49%）
- **测试误差 MSE = 133.75** → 平均预测误差 ≈ 11.56亿元（相对误差2.41%）

**原因**:
1. 训练误差总是比测试误差小（模型在训练数据上优化过）
2. 测试误差反映真实泛化能力
3. 数据集很小（50个训练样本），容易出现分布差异
4. 相对误差只有2.41%，对于房价预测是可接受的

详细分析请查看 `RESULTS_ANALYSIS.md` 和运行 `python analyze_data.py`

### 逻辑回归关键发现

1. **收敛性**：对数似然值随迭代次数单调递增，表明优化正确
2. **分类性能**：在测试集上达到100%准确率（完美分类）
3. **训练集大小影响**：
   - 训练集增大 → 训练误差上升（更难完美拟合）
   - 训练集增大 → 测试误差下降（泛化能力增强）

## 注意事项

1. 程序运行时间较长（特别是练习一的1,500,000次迭代），请耐心等待
2. 所有图表自动保存到 `analysis/` 目录
3. 设置了随机种子以确保结果可重复
4. 实现了数值稳定性处理（sigmoid函数裁剪、对数计算保护等）
